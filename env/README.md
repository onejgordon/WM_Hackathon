# Install custom Gym environment:
You need to pre-install the custom Gym environments contained in this repository.

```
cd WM_Hackathon/env
pip install --ignore-installed --no-deps -e .
```

# Tasks
3 tasks are included in this repository:

1. Simple task - a grid world, in which the agent must move to a goal position. Then the goal moves to a new position.
2. Match-to-Sample (M2S) task. An agent is shown a sample, and then given a choice of two other samples to match it. The correct match will be the same in one attribute, such as colour or shape. False matches vary in the necessary attribute. The relevant attribute varies from game to game, and the correct logic is learned by watching a tutor play the game first. 
3. Delayed Match-to-Sample (DM2S). As M2S, but the sample to match is displayed briefly before the possible choices are displayed. The sample to match must be memorized. The rules of the game 

# Agent architecture
The Agent comprises an active vision system, a pre-trained visual cortex, and other brain regions trained by Reinforcement Learning.

# Human gameplay
These tests allow you to evaluate custom env gameplay manually. This helps you get a feel for the games and rules.

## Simple RL task
`python keyboard_agent.py simple-v0 configs/simple_env_human.json`

## Match to Sample task
`python keyboard_agent.py m2s-v0 configs/m2s_env.json `

## Delayed Match to Sample task
`python keyboard_agent.py dm2s-v0 configs/dm2s_env.json `

# Pretraining phase
You can pretrain parts of a model on a custom environment without rewards. This has two parts: First, pre-generate some data from the environment. Second, pretrain model modules on this data.

## Pre-generating data
Data is pre-generated by running an environment with a (by default) random action policy. The observations are recorded for later iid sampling. 

Examples:

### Match to Sample
`python generate.py m2s-v0 configs/m2s_env.json 2000 ./data/gen`

### Delayed Match to Sample
`python generate.py dm2s-v0 configs/dm2s_env.json 2000 ./data/gen`

## Pre-training modules
Each module that requires pretraining should be pretrained separately. Examples:

`python pretrain_visual_cortex.py --config ./configs/pretrain_fovea.json --env dm2s-v0 --env-config ./configs/dm2s_env.json --env-data-dir=./data/gen --env-obs-key=fovea --model-file=./data/pretrain/fovea.pt --epochs 10`

`python pretrain_visual_cortex.py --config ./configs/pretrain_peripheral.json --env dm2s-v0 --env-config ./configs/dm2s_env.json --env-data-dir=./data/gen --env-obs-key=peripheral --model-file=./data/pretrain/peripheral.pt --epochs 10`

`python pretrain_visual_cortex.py --config ./configs/pretrain_full.json --env dm2s-v0 --env-config ./configs/dm2s_env.json --env-data-dir=./data/gen --env-obs-key=full --model-file=./data/pretrain/full.pt --epochs 10`

To view the output of pretraining, you can examine the tensorboard output in the ./run directory.

To start tensorboard in this folder, use a command such as:

`tensorboard --logdir=. --port=6008 --samples_per_plugin images=200`

## Training the RL agent
This actually trains the Reinforcement-Learning parts of the Agent on the task. It reloads pretrained networks for posterior cortex and other brain modules that are not trained rapidly or via RL.

Command structure: 
`python train_stub_agent.py TASK_ENV TASK_ENV_CONFIG_FILE STUB_ENV_CONFIG_FILE AGENT_CONFIG_FILE`

Note that STUB_ENV_CONFIG_FILE should configure and reload pretrained networks (e.g. Cortex). 
 
AGENT_CONFIG_FILE should configure the RL-trained agent network[s].

Example:

`python train_stub_agent.py m2s-v0 configs/m2s_env.json configs/stub_model_full.json configs/stub_agent_full.json`

This command will train the default stub agent on the full image, using a pretrained visual cortex, and learn via RL to play the M2S task.

# Stub Agent Validation 
The Stub Agent is provided as a working baseline for you to build on. You can change any part you wish, but the intention is for you to focus on adding working memory to enable the agent to solve the DM2S task.

This section describes the steps that can be performed to validate the functionality of the provided stub agent, and that the architecture as a whole can solve tasks. The stub agent, with a view of the full image, is pretrained and then trained via RL on the M2S task.

1. `python keyboard_agent.py m2s-v0 configs/m2s_env.json` 
2. `python generate.py m2s-v0 configs/m2s_env.json 2000 ./data/gen_m2s`
3. `python pretrain_visual_cortex.py --config ./configs/pretrain_full.json --env m2s-v0 --env-config ./configs/m2s_env.json --env-data-dir=./data/gen_m2s --env-obs-key=full --model-file=./data/pretrain/full.pt --epochs 7`
4. `python train_stub_agent.py m2s-v0 configs/m2s_env.json configs/stub_model_full.json configs/stub_agent_full.json`

# Unit tests
These tests verify the performance of the provided stubs.

## Simple Reinforcement Learning task
`python train_simple_agent.py simple-v0 configs/simple_env_machine.json configs/simple_agent_model.json`

Example output:
` 71 reward   0.00/  1.87/  4.00 len 389.30 saved tmp/simple/checkpoint_71/checkpoint-71`

The rewards are min/mean/max per epoch.
Should optimize to around 0/4/5.
